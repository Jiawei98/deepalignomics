{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import os.path\n",
    "import pdb\n",
    "cuda = torch.device('cuda') \n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from random import sample\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, X_train, y_train, opt, criterion, sim, batch_size=200):\n",
    "    model.train()\n",
    "    sim = sim\n",
    "    losses = []\n",
    "    for beg_i in range(0, X_train.size(0), batch_size):\n",
    "        x_batch = X_train[beg_i:beg_i + batch_size, :]\n",
    "        y_batch = y_train[beg_i:beg_i + batch_size]\n",
    "        x_batch = Variable(x_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        y_hat = model(x_batch.float())\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        #print(loss)\n",
    "        reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in net.fc1.named_parameters():\n",
    "            if 'weight' in name:\n",
    "#                 reg = torch.norm(reg + param @ sim.float() @ param.T, 2)\n",
    "                M = .5 * ((torch.eye(feature_dim) - sim).T @ (torch.eye(feature_dim) - sim)) + .5 * torch.eye(feature_dim)\n",
    "                reg = torch.norm(reg + param @ M.float() @ param.T, 2)\n",
    "                loss += reg\n",
    "#         for param in model.parameters():\n",
    "#             loss += .1 * torch.sum(torch.abs(param))\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_noreg(model, X_train, y_train, opt, criterion, sim, batch_size=200):\n",
    "    model.train()\n",
    "    sim = sim\n",
    "    losses = []\n",
    "    for beg_i in range(0, X_train.size(0), batch_size):\n",
    "        x_batch = X_train[beg_i:beg_i + batch_size, :]\n",
    "        y_batch = y_train[beg_i:beg_i + batch_size]\n",
    "        x_batch = Variable(x_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        y_hat = model(x_batch.float())\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        #print(loss)\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 200)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(50,5)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.prelu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Efeature = pd.read_csv('../data/efeature_filtered.csv',index_col=0)\n",
    "geneExp = pd.read_csv('../data/expMat_filtered.csv',index_col=0)\n",
    "label = pd.read_csv('../data/label_visual.csv')\n",
    "feature_new = pd.read_csv(\"../data/deepmanreg_latent.csv\",index_col=0)\n",
    "ma_latent = pd.read_csv(\"../data/ma_latent.csv\").to_numpy()\n",
    "cca_latent = pd.read_csv(\"../data/cca_latent.csv\").to_numpy()\n",
    "matcher_latent = pd.read_csv(\"../data/matcher_latent.csv\").to_numpy()\n",
    "\n",
    "#x1_np = preprocessing.scale(np.log(geneExp+1).to_numpy(),axis=1)\n",
    "#x2_np = preprocessing.scale(Efeature.T.to_numpy(),axis=1)\n",
    "\n",
    "x1_np = np.log(geneExp+1).to_numpy()\n",
    "x2_np = preprocessing.scale(Efeature.T.to_numpy(),axis=1)\n",
    "\n",
    "data = np.concatenate((x1_np,x2_np), axis=0).T\n",
    "print(data.shape)\n",
    "\n",
    "distance_matrix = sp.spatial.distance_matrix(feature_new, feature_new)\n",
    "#sim_mat = 1/(1+distance_matrix)\n",
    "sim_mat = 1/np.exp(distance_matrix)\n",
    "sim_mat[sim_mat > np.percentile(sim_mat,50)] = 1\n",
    "sim_mat[sim_mat != 1] = 0\n",
    "sim = torch.from_numpy(sim_mat)\n",
    "sim = sim.fill_diagonal_(0).float()\n",
    "\n",
    "distance_matrix2 = sp.spatial.distance_matrix(ma_latent,ma_latent)\n",
    "sim_mat2 = 1/(1+distance_matrix2)\n",
    "sim_mat2[sim_mat2 > np.percentile(sim_mat2,50)] = 1\n",
    "sim_mat2[sim_mat2 != 1] = 0\n",
    "sim2 = torch.from_numpy(sim_mat2)\n",
    "sim2 = sim2.fill_diagonal_(0).float()\n",
    "\n",
    "distance_matrix3 = sp.spatial.distance_matrix(cca_latent,cca_latent)\n",
    "sim_mat3 = 1/(1+distance_matrix3)\n",
    "sim_mat3[sim_mat3 > np.percentile(sim_mat3,50)] = 1\n",
    "sim_mat3[sim_mat3 != 1] = 0\n",
    "sim3 = torch.from_numpy(sim_mat3)\n",
    "sim3 = sim3.fill_diagonal_(0).float()\n",
    "\n",
    "distance_matrix4 = sp.spatial.distance_matrix(matcher_latent,matcher_latent)\n",
    "sim_mat4 = 1/(1+distance_matrix4)\n",
    "sim_mat4[sim_mat4 > np.percentile(sim_mat4,50)] = 1\n",
    "sim_mat4[sim_mat4 != 1] = 0\n",
    "sim4 = torch.from_numpy(sim_mat4)\n",
    "sim4 = sim4.fill_diagonal_(0).float()\n",
    "\n",
    "labels = label.iloc[:,3].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "acc_reg = []\n",
    "acc_noreg = []\n",
    "acc_ma = []\n",
    "acc_cca = []\n",
    "acc_matcher = []\n",
    "\n",
    "pred_reg = []\n",
    "pred_noreg = []\n",
    "pred_ma = []\n",
    "pred_cca = []\n",
    "pred_matcher = []\n",
    "\n",
    "#weights = [448,719,43,214,1456,684]\n",
    "#weights = {3: 1176, 1: 1097, 4: 734, 2: 385, 0: 262}\n",
    "#weights = [1 - (x / sum(weights)) for x in weights]\n",
    "#weights = torch.tensor(weights, dtype=torch.float32)\n",
    "X = data\n",
    "y = pd.factorize(labels,sort=True)[0]\n",
    "#X_train_vali, X_test, y_train_vali, y_test = train_test_split(X,y,test_size=0.2, \n",
    "#                                                        random_state=0, stratify = y)\n",
    "#X_test, y_test = SMOTE.fit_resample(X_test, y_test)\n",
    "#print(\"After oversampling test: \",Counter(y_test))\n",
    "#X, y = SMOTE.fit_resample(X, y)\n",
    "for i in range(100):\n",
    "    #X_train, X_vali, y_train, y_vali = train_test_split(X_train_vali,y_train_vali,test_size=0.2, \n",
    "    #                                                    random_state=i, stratify = y_train_vali)\n",
    "    X_train, X_vali, y_train, y_vali = train_test_split(X,y,test_size=0.2,\n",
    "                                                        random_state=i, stratify = y)\n",
    "    \n",
    "    #print(\"Before oversampling: \",Counter(y_train))\n",
    "\n",
    "    # fit and apply the transform\n",
    "    X_train, y_train = SMOTE(random_state=i).fit_resample(X_train, y_train)\n",
    "    #X_vali, y_vali = SMOTE.fit_resample(X_vali, y_vali)\n",
    "    # summarize class distribution\n",
    "    if i == 0:\n",
    "        print(\"After oversampling train: \",Counter(y_train))\n",
    "        print(\"Without oversampling Validation: \",Counter(y_vali))\n",
    "    \n",
    "    torch.manual_seed(i)\n",
    "    \n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    \n",
    "    # no regulized network\n",
    "    feature_dim = 1041\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch_noreg(net, X_train, y_train, opt, criterion, sim)\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (unregularized):\",correct_pred)\n",
    "    acc_noreg.append(np.mean(correct_pred)) \n",
    "    pred_noreg.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    \n",
    "    # regulized network   \n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (regularized):\",correct_pred)\n",
    "    acc_reg.append(np.mean(correct_pred))\n",
    "    pred_reg.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "\n",
    "    # manifold alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (manifold alignment):\",correct_pred)\n",
    "    acc_ma.append(np.mean(correct_pred))\n",
    "    pred_ma.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    \n",
    "    #cca alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (cca):\",correct_pred)\n",
    "    acc_cca.append(np.mean(correct_pred))\n",
    "    pred_cca.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "\n",
    "    #matcher alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (matcher):\",correct_pred)\n",
    "    acc_matcher.append(np.mean(correct_pred))\n",
    "    pred_matcher.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = np.array(pred_reg)\n",
    "pred_noreg = np.array(pred_noreg)\n",
    "pred_ma= np.array(pred_ma)\n",
    "pred_cca = np.array(pred_cca)\n",
    "pred_matcher = np.array(pred_matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../data/visual/pred_reg.csv\", pred_reg, delimiter=\",\")\n",
    "np.savetxt(\"../data/visual/pred_noreg.csv\", pred_noreg, delimiter=\",\")\n",
    "np.savetxt(\"../data/visual/pred_ma.csv\", pred_ma, delimiter=\",\")\n",
    "np.savetxt(\"../data/visual/pred_cca.csv\", pred_cca, delimiter=\",\")\n",
    "np.savetxt(\"../data/visual/pred_matcher.csv\", pred_matcher, delimiter=\",\")\n",
    "np.savetxt(\"../data/visual/y_vali.csv\", y_vali, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ks_2samp(acc_reg,acc_matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(acc_matcher,[2.5,50,97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "reg = plt.hist(acc_noreg, bins=30, range=[0.2, 0.6],color = \"#FC766AFF\",alpha=0.7,density=False, histtype='bar', ec='black')\n",
    "noreg = plt.hist(acc_reg, bins=30, alpha=0.7,color = \"#5B84B1FF\",density=False, histtype='bar', ec='black')\n",
    "plt.xlabel(\"Accuracy\", size=14)\n",
    "plt.ylabel(\"Probability\", size=14)\n",
    "#plt.title(\"Histograms for validation accuracy\")\n",
    "plt.legend(['without regularization', 'with regularization'])\n",
    "plt.savefig(\"../figure/hist_validation.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(acc_noreg, hist = False, kde = True, norm_hist=True,kde_kws = { 'linewidth': 3})\n",
    "sns.distplot(acc_reg, hist = False, kde = True, norm_hist=True,kde_kws = { 'linewidth': 3})\n",
    "sns.distplot(acc_ma, hist = False, kde = True, norm_hist=True,kde_kws = { 'linewidth': 3})\n",
    "sns.distplot(acc_cca, hist = False, kde = True, norm_hist=True,kde_kws = { 'linewidth': 3})\n",
    "sns.distplot(acc_matcher, hist = False, kde = True, norm_hist=True,kde_kws = { 'linewidth': 3})\n",
    "plt.xlabel(\"Accuracy\", size=14)\n",
    "plt.ylabel(\"Density\", size=14)\n",
    "#plt.title(\"Histograms for validation accuracy\")\n",
    "plt.legend(['without regularization', 'with regularization'])\n",
    "plt.savefig(\"../figure/hist_validation.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network for all data\n",
    "X = data\n",
    "y = pd.factorize(labels,sort=True)[0]\n",
    "X, y = SMOTE(random_state = 0).fit_resample(X, y)\n",
    "print(\"After oversampling whole: \",Counter(y))\n",
    "\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "net = Net()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_losses = []\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    e_losses += train_epoch(net, X, y, opt, criterion, sim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_tensor_test = X.float()#.to(device)\n",
    "    net.eval()\n",
    "    yhat = net(x_tensor_test)\n",
    "y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "_, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "correct_pred = np.mean([float(y_pred_tags[i] == y[i]) for i in range(len(y))])\n",
    "print(\"Train Accuracy (regularized):\",correct_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "real = label_binarize(y, classes=range(5))\n",
    "pred = y_pred_softmax.detach().numpy()\n",
    "#pred = net(X_train.float()).detach().numpy()\n",
    "for i in range(5):\n",
    "    fpr[i], tpr[i], _ = roc_curve(real[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(real.ravel(), pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "layer=[\"L1\",\"L2/3\",\"L4\",\"L5\",\"L6\"]\n",
    "plt.figure(figsize=(8,12))\n",
    "ax=plt.subplot(211)\n",
    "sns.distplot(acc_noreg, hist = False, kde = True,kde_kws = { 'linewidth': 3,'cumulative': True})\n",
    "sns.distplot(acc_reg, hist = False, kde = True,kde_kws = { 'linewidth': 3,'cumulative': True})\n",
    "sns.distplot(acc_ma, hist = False, kde = True,kde_kws = { 'linewidth': 3,'cumulative': True})\n",
    "sns.distplot(acc_cca, hist = False, kde = True,kde_kws = { 'linewidth': 3,'cumulative': True})\n",
    "sns.distplot(acc_matcher, hist = False, kde = True,kde_kws = { 'linewidth': 3,'cumulative': True})\n",
    "plt.xlabel(\"Accuracy\", size=14)\n",
    "plt.ylabel(\"Probability\", size=14)\n",
    "#plt.title(\"Histograms for validation accuracy\")\n",
    "plt.legend(['without regularization', 'DeepManReg',\"Linear Manifold Alignment\",\"Canonical-correlation Analysis\",\"Matcher\"])\n",
    "#plt.savefig(\"../figure/hist_validation.eps\")\n",
    "ax.text(-0.1, 1.05, \"A\", transform=ax.transAxes,\n",
    "      fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "lw = 2\n",
    "ax=plt.subplot(212)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "for i, color in zip(range(5), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of '+layer[i]+' (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', size=14)\n",
    "plt.ylabel('True Positive Rate', size=14)\n",
    "plt.title('')\n",
    "plt.legend(loc=\"lower right\")\n",
    "ax.text(-0.1, 1.05, \"B\", transform=ax.transAxes,\n",
    "      fontsize=25, fontweight='bold', va='top', ha='right')\n",
    "plt.savefig(\"../figure/Roc_curve.eps\")\n",
    "plt.savefig(\"../figure/Roc_curve.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify t-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 200)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(50,6)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.prelu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "    \n",
    "acc_reg_t = []\n",
    "acc_noreg_t = []\n",
    "acc_ma_t = []\n",
    "acc_cca_t = []\n",
    "acc_matcher_t = []\n",
    "\n",
    "pred_reg_t = []\n",
    "pred_noreg_t = []\n",
    "pred_ma_t = []\n",
    "pred_cca_t = []\n",
    "pred_matcher_t = []\n",
    "\n",
    "#weights = [448,719,43,214,1456,684]\n",
    "#weights = {3: 1176, 1: 1097, 4: 734, 2: 385, 0: 262}\n",
    "#weights = [1 - (x / sum(weights)) for x in weights]\n",
    "#weights = torch.tensor(weights, dtype=torch.float32)\n",
    "X = data\n",
    "labels = label.iloc[:,1].to_numpy()\n",
    "y = pd.factorize(labels,sort=True)[0]\n",
    "#X_train_vali, X_test, y_train_vali, y_test = train_test_split(X,y,test_size=0.2, \n",
    "#                                                        random_state=0, stratify = y)\n",
    "#X_test, y_test = SMOTE.fit_resample(X_test, y_test)\n",
    "#print(\"After oversampling test: \",Counter(y_test))\n",
    "#X, y = SMOTE.fit_resample(X, y)\n",
    "for i in range(100):\n",
    "    #X_train, X_vali, y_train, y_vali = train_test_split(X_train_vali,y_train_vali,test_size=0.2, \n",
    "    #                                                    random_state=i, stratify = y_train_vali)\n",
    "    X_train, X_vali, y_train, y_vali = train_test_split(X,y,test_size=0.2,\n",
    "                                                        random_state=i, stratify = y)\n",
    "    \n",
    "    #print(\"Before oversampling: \",Counter(y_train))\n",
    "\n",
    "    # fit and apply the transform\n",
    "    X_train, y_train = SMOTE(random_state=i).fit_resample(X_train, y_train)\n",
    "    #X_vali, y_vali = SMOTE.fit_resample(X_vali, y_vali)\n",
    "    # summarize class distribution\n",
    "    if i == 0:\n",
    "        print(\"After oversampling train: \",Counter(y_train))\n",
    "        print(\"Without oversampling Validation: \",Counter(y_vali))\n",
    "    \n",
    "    torch.manual_seed(i)\n",
    "    \n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    \n",
    "    # no regulized network\n",
    "    feature_dim = 1041\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch_noreg(net, X_train, y_train, opt, criterion, sim)\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (unregularized):\",correct_pred)\n",
    "    acc_noreg_t.append(np.mean(correct_pred)) \n",
    "    pred_noreg_t.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    \n",
    "    # regulized network   \n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (regularized):\",correct_pred)\n",
    "    acc_reg_t.append(np.mean(correct_pred))\n",
    "    pred_reg_t.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "\n",
    "    # manifold alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (manifold alignment):\",correct_pred)\n",
    "    acc_ma_t.append(np.mean(correct_pred))\n",
    "    pred_ma_t.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    \n",
    "    #cca alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (cca):\",correct_pred)\n",
    "    acc_cca_t.append(np.mean(correct_pred))\n",
    "    pred_cca_t.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "\n",
    "    #matcher alignment\n",
    "    net = Net()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    e_losses = []\n",
    "    num_epochs = 20\n",
    "    for e in range(num_epochs):\n",
    "        e_losses += train_epoch(net, X_train, y_train, opt, criterion, sim3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_tensor_test = torch.from_numpy(X_vali).float()#.to(device)\n",
    "        net.eval()\n",
    "        yhat = net(x_tensor_test)\n",
    "    y_pred_softmax = torch.log_softmax(yhat, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = np.mean([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])\n",
    "    print(\"Round\",i,\"Test Accuracy (matcher):\",correct_pred)\n",
    "    acc_matcher_t.append(np.mean(correct_pred))\n",
    "    pred_matcher_t.append([float(y_pred_tags[i] == y_vali[i]) for i in range(len(y_vali))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(acc_noreg_t,[2.5,50,97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acc_reg_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

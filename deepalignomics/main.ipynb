{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sd\n",
    "\n",
    "from distance import SquaredL2\n",
    "from neighborhood import neighbor_graph, laplacian\n",
    "from correspondence import Correspondence\n",
    "from stiefel import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from datareader import *\n",
    "\n",
    "import os.path\n",
    "\n",
    "import pdb\n",
    "cuda = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines the neural network\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H1)\n",
    "        self.linear3 = torch.nn.Linear(H1, H1)\n",
    "        self.linear4 = torch.nn.Linear(H1, D_out)\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        y_pred = self.linear4(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute the projected gradient onto the tangent space of Stiefel Manifold\"\"\"\n",
    "\n",
    "def skew(M,Z):\n",
    "    ''' return the skew-symmetric part of $M^T Z$'''\n",
    "    return 0.5 * (M.t()@Z - Z.t()@M)\n",
    "\n",
    "def proj_stiefel(M,Z):\n",
    "    ''' M is a d-by-r point on the stiefel manifold, defining a tangent\n",
    "    space $T_M \\mathcal{O}^{d \\times r}$\n",
    "    $Z \\in \\mathbb{R}^{d\\times r}$ is an arbitrary point \n",
    "    we would like to project onto the '''\n",
    "    MskewMTZ = M@skew(M,Z)\n",
    "    IMMTZ = (torch.eye(len(M)) - M@M.t())@Z\n",
    "    return MskewMTZ + IMMTZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (1814, 43885)\n",
      "(10000, 1556)\n",
      "(10000, 258)\n"
     ]
    }
   ],
   "source": [
    "# Load patient data from excel file if there isn't any\n",
    "data_file = './data/DER-01_PEC_Gene_expression_matrix_normalized.txt'\n",
    "metadata_file = './data/PEC_Capstone_clinical_meta - PEC_Capstone_clinical_meta.csv'\n",
    "if (not os.path.isfile('./data/gene_data.npy')):\n",
    "    \n",
    "    x, y, patient_id, feature_name = read_data(data_file, metadata_file)\n",
    "    \n",
    "    np.save('./data/gene_data.npy', x)\n",
    "    np.save('./data/label.npy', y)\n",
    "    np.save('./data/patient_id.npy', patient_id)\n",
    "    np.save('./data/feature_name.npy', feature_name)\n",
    "else:\n",
    "    x = np.load('./data/gene_data.npy')\n",
    "    y = np.load('./data/label.npy')\n",
    "    patient_id = np.load('./data/patient_id.npy')\n",
    "    feature_name = np.load('./data/feature_name.npy')\n",
    "\n",
    "print('Shape of data: ', x.shape)\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "x1_np = x[y == 0, :].T\n",
    "x2_np = x[y == 1, :].T\n",
    "\n",
    "# Take out a small bunch for debugging purpose\n",
    "x1_np = x1_np[0:10000, :]\n",
    "x2_np = x2_np[0:10000, :]\n",
    "\n",
    "N, H1, H2, D_out = x1_np.shape[0], 1024, 512, 10\n",
    "\n",
    "D_in1 = x1_np.shape[1]\n",
    "D_in2 = x2_np.shape[1]\n",
    "\n",
    "print(x1_np.shape)\n",
    "print(x2_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_neighbor(x, bsize, k):\n",
    "    \"\"\"my_neighbor\n",
    "    :param x: n x d numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    #x = torch.from_numpy(x).cuda()\n",
    "    n = x.shape[0]\n",
    "    n_ite = int(np.ceil(n*1.0 / bsize))\n",
    "    \n",
    "    adj_matrix = np.zeros((n, n), dtype=np.int32)\n",
    "    \n",
    "    for i in range(n_ite):\n",
    "        if (i % 100 == 0):\n",
    "            print(i)\n",
    "        \n",
    "        batch_range = range(i*bsize, (i+1)*bsize)\n",
    "        if (batch_range[-1] > n):\n",
    "            batch_range = range(i*bsize, n)\n",
    "        \n",
    "        x_i = x[batch_range, :]\n",
    "        \n",
    "        d_i = sd.cdist(x_i, x)\n",
    "        \n",
    "        k_nearest_i = np.argsort(d_i, axis=1)[:, 1:(k+1)]\n",
    "        #k_nearest_matrix[batch_range, :] = k_nearest_i\n",
    "        for j in batch_range:\n",
    "            #pdb.set_trace()\n",
    "            #ind = np.array(list(zip(batch_range, k_nearest_i[j,:])))\n",
    "            try:\n",
    "                adj_matrix[j, k_nearest_i[j%bsize,:]] = 1\n",
    "                adj_matrix[k_nearest_i[j%bsize,:], j] = 1\n",
    "            except Exception as e:\n",
    "                pdb.set_trace()\n",
    "    \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load computed neighbor graph...\n",
      "Computing Laplacian...\n",
      "Finished computing Laplacian\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.isfile('./data/adj1_%d.npy' % N)):\n",
    "    print('Computing neighbor graph...')\n",
    "    #adj1 = neighbor_graph(x1_np, k=5)\n",
    "    #adj2 = neighbor_graph(x2_np, k=5)\n",
    "    \n",
    "    adj1 = my_neighbor(x1_np, bsize=10, k=5)\n",
    "    adj2 = my_neighbor(x2_np, bsize=10, k=5)\n",
    "    \n",
    "    print('Finished computing neighbor graph')\n",
    "    np.save('./data/adj1_%d.npy' % N, adj1)\n",
    "    np.save('./data/adj2_%d.npy' % N, adj2)\n",
    "else:\n",
    "    print('Load computed neighbor graph...')\n",
    "    adj1 = np.load('./data/adj1_%d.npy' % N)\n",
    "    adj2 = np.load('./data/adj2_%d.npy' % N)\n",
    "\n",
    "corr = Correspondence(matrix=np.eye(N))\n",
    "\n",
    "w = np.block([[corr.matrix(),adj1],\n",
    "              [adj2, corr.matrix()]])\n",
    "\n",
    "print('Computing Laplacian...')\n",
    "L_np = laplacian(w, normed=True)\n",
    "L = torch.from_numpy(L_np.astype(np.float32)).cuda()\n",
    "print('Finished computing Laplacian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001] Loss: 0.13100\n",
      "[0011] Loss: 0.10538\n",
      "[0021] Loss: 0.09376\n",
      "[0031] Loss: 0.11597\n",
      "[0041] Loss: 0.12343\n",
      "[0051] Loss: 0.09593\n",
      "[0061] Loss: 0.09286\n",
      "[0071] Loss: 0.06392\n",
      "[0081] Loss: 0.06423\n",
      "[0091] Loss: 0.06438\n",
      "[0101] Loss: 0.05567\n",
      "[0111] Loss: 0.09116\n",
      "[0121] Loss: 0.06549\n",
      "[0131] Loss: 0.05904\n",
      "[0141] Loss: 0.05573\n",
      "[0151] Loss: 0.07230\n",
      "[0161] Loss: 0.08181\n",
      "[0171] Loss: 0.06186\n",
      "[0181] Loss: 0.08137\n",
      "[0191] Loss: 0.07379\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model1 = Net(D_in1, H1, D_out)\n",
    "model2 = Net(D_in2, H1, D_out)\n",
    "\n",
    "model1.cuda()\n",
    "model2.cuda()\n",
    "\n",
    "# Construct an Optimizer\n",
    "params = list(model1.parameters()) + list(model2.parameters())\n",
    "#optimizer = torch.optim.SGD(params, lr = 0.004)\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "\n",
    "random_stiefel = rand_stiefel(N*2, D_out)\n",
    "\n",
    "# Putting x1 and x2 to tensor\n",
    "x1 = torch.from_numpy(x1_np).cuda()\n",
    "x2 = torch.from_numpy(x2_np).cuda()\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "n_ite =int(np.ceil(N*2.0 / batch_size));\n",
    "n_epoch = 200\n",
    "\n",
    "for e in range(n_epoch):\n",
    "    for t in range(n_ite):\n",
    "        batch_range = range(t*batch_size, (t+1)*batch_size)\n",
    "        if (batch_range[-1] > N*2):\n",
    "            batch_range = range(t*batch_size, N*2)\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y1_pred = model1(x1)\n",
    "        y2_pred = model2(x2)\n",
    "\n",
    "        outputs = torch.cat((y1_pred, y2_pred), 0)\n",
    "        outputs = outputs[batch_range]\n",
    "\n",
    "        loss = torch.norm(outputs - random_stiefel[batch_range, :])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (e % 10 == 0):\n",
    "        print('[%04d] Loss: %.5f' % (e+1, loss.cpu().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 9.628013610839844\n",
      "20 9.608041763305664\n",
      "30 9.6057767868042\n",
      "40 9.624540328979492\n",
      "50 9.59405517578125\n",
      "60 9.599438667297363\n",
      "70 9.60513687133789\n",
      "80 9.612357139587402\n",
      "90 9.602792739868164\n",
      "100 9.609492301940918\n",
      "110 9.61546516418457\n",
      "120 9.610350608825684\n",
      "130 9.600461959838867\n",
      "140 9.603946685791016\n",
      "150 9.587674140930176\n",
      "160 9.606496810913086\n",
      "170 9.59586238861084\n",
      "180 9.581197738647461\n",
      "190 9.601890563964844\n",
      "200 9.576210021972656\n",
      "210 9.609225273132324\n",
      "220 9.573551177978516\n",
      "230 9.614559173583984\n",
      "240 9.59571361541748\n",
      "250 9.585641860961914\n",
      "260 9.57463550567627\n",
      "270 9.593141555786133\n",
      "280 9.591930389404297\n",
      "290 9.59908676147461\n",
      "300 9.581263542175293\n",
      "310 9.601405143737793\n",
      "320 9.584795951843262\n",
      "330 9.587654113769531\n",
      "340 9.568805694580078\n",
      "350 9.57101821899414\n",
      "360 9.591824531555176\n",
      "370 9.587491035461426\n",
      "380 9.570846557617188\n",
      "390 9.578156471252441\n",
      "400 9.570921897888184\n",
      "410 9.583796501159668\n",
      "420 9.552523612976074\n",
      "430 9.575945854187012\n",
      "440 9.55333423614502\n",
      "450 9.570051193237305\n",
      "460 9.558344841003418\n",
      "470 9.575005531311035\n",
      "480 9.549882888793945\n",
      "490 9.56810188293457\n",
      "500 9.554265022277832\n",
      "510 9.55324935913086\n",
      "520 9.55251407623291\n",
      "530 9.57292366027832\n",
      "540 9.551565170288086\n",
      "550 9.555378913879395\n",
      "560 9.570961952209473\n",
      "570 9.567567825317383\n",
      "580 9.575536727905273\n",
      "590 9.561899185180664\n",
      "600 9.565620422363281\n",
      "610 9.544173240661621\n",
      "620 9.547487258911133\n",
      "630 9.551868438720703\n",
      "640 9.57520866394043\n",
      "650 9.574605941772461\n",
      "660 9.551773071289062\n",
      "670 9.56218147277832\n",
      "680 9.555535316467285\n",
      "690 9.558070182800293\n",
      "700 9.546146392822266\n",
      "710 9.552799224853516\n",
      "720 9.546043395996094\n",
      "730 9.559722900390625\n",
      "740 9.565340042114258\n",
      "750 9.54922103881836\n",
      "760 9.557435989379883\n",
      "770 9.54998779296875\n",
      "780 9.546789169311523\n",
      "790 9.552799224853516\n",
      "800 9.56362533569336\n",
      "810 9.546013832092285\n",
      "820 9.549983978271484\n",
      "830 9.550958633422852\n",
      "840 9.549731254577637\n",
      "850 9.562674522399902\n",
      "860 9.541338920593262\n",
      "870 9.553414344787598\n",
      "880 9.548788070678711\n",
      "890 9.575874328613281\n",
      "900 9.570035934448242\n",
      "910 9.551774978637695\n",
      "920 9.543818473815918\n",
      "930 9.552631378173828\n",
      "940 9.557524681091309\n",
      "950 9.571646690368652\n",
      "960 9.550105094909668\n",
      "970 9.550475120544434\n",
      "980 9.544927597045898\n",
      "990 9.555864334106445\n",
      "1000 9.545319557189941\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "\n",
    "# Construct an Optimizer\n",
    "params = list(model1.parameters()) + list(model2.parameters())\n",
    "optimizer = torch.optim.SGD(params, lr = 0.02)\n",
    "#optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "batch_size = 128\n",
    "n_epoch = 500\n",
    "n_ite = int(np.ceil(n_epoch * 1.0 / batch_size))\n",
    "\n",
    "for e in range(500):\n",
    "    accum_grad = torch.zeros((256, 10)).cuda()\n",
    "    for t in range(n_ite):\n",
    "        batch_range = range(t*batch_size, (t+1)*batch_size)\n",
    "        if (batch_range[-1] > N):\n",
    "            batch_range = range(t*batch_size, N)\n",
    "        L_batch_range = np.concatenate((batch_range, np.asarray(batch_range) + N), 0)\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        try:\n",
    "            y1_pred = model1(x1[batch_range, :])\n",
    "            y2_pred = model2(x2[batch_range, :])\n",
    "        except Exception as e:\n",
    "            pdb.set_trace()\n",
    "\n",
    "        outputs = torch.cat((y1_pred, y2_pred), 0)\n",
    "\n",
    "        # Project the output onto Stiefel Manifold\n",
    "        u, s, v = torch.svd(outputs, some=True)\n",
    "        proj_outputs = u@v.t()\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = torch.trace(proj_outputs.t()@L[L_batch_range,:][:, L_batch_range]@proj_outputs)\n",
    "0\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        proj_outputs.retain_grad()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Project the (Euclidean) gradient onto the tangent space of Stiefel Manifold (to get Rimannian gradient)\n",
    "        rgrad = proj_stiefel(proj_outputs, proj_outputs.grad) \n",
    "        accum_grad += rgrad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Backpropogate the Rimannian gradient w.r.t proj_outputs\n",
    "    proj_outputs.backward(accum_grad)\n",
    "    accum_grad[:] = 0\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if (e % 10 == 9):\n",
    "        print(e+1, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

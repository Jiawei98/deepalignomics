{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from distance import SquaredL2\n",
    "from neighborhood import neighbor_graph, laplacian\n",
    "from correspondence import Correspondence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines the neural network\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1_sigmoid = self.linear1(x).sigmoid()\n",
    "        h2_sigmoid = self.linear2(h1_sigmoid).sigmoid()\n",
    "        y_pred = self.linear3(h2_sigmoid)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H1, H2, D_out = 64, 1000, 500, 100, 10\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = Net(D_in, H1, H2, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold the 2 inputs\n",
    "x1 = torch.randn(N, D_in)\n",
    "x2 = torch.randn(N, D_in)\n",
    "\n",
    "# Compute Laplacian of the join datasets\n",
    "# To-do: Write these functions in PyTorch instead of Numpy\n",
    "x1_np = x1.numpy()\n",
    "x2_np = x2.numpy()\n",
    "\n",
    "adj1 = neighbor_graph(x1_np, k=5)\n",
    "adj2 = neighbor_graph(x2_np, k=5)\n",
    "\n",
    "corr = Correspondence(matrix=np.eye(N))\n",
    "\n",
    "w = np.block([[corr.matrix(),adj1],\n",
    "              [adj2, corr.matrix()]])\n",
    "\n",
    "L_np = laplacian(w, normed=True)\n",
    "L = torch.from_numpy(L_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.109815318462065\n",
      "1 8.361776043498823\n",
      "2 7.765870151576243\n",
      "3 7.288484336409264\n",
      "4 6.899069467437523\n",
      "5 6.5781753687372095\n",
      "6 6.317899712799942\n",
      "7 6.111191549940019\n",
      "8 5.948485240747778\n",
      "9 5.819532041773905\n",
      "10 5.7156335869883055\n",
      "11 5.630303688570077\n",
      "12 5.55895471445482\n",
      "13 5.498386832147433\n",
      "14 5.446356942272679\n",
      "15 5.401265197819636\n",
      "16 5.361938741599147\n",
      "17 5.3274864717339145\n",
      "18 5.297204454071539\n",
      "19 5.270516436686357\n",
      "20 5.246937775837263\n",
      "21 5.226053878993947\n",
      "22 5.20750708991373\n",
      "23 5.190988027453793\n",
      "24 5.176229215084842\n",
      "25 5.162999819113034\n",
      "26 5.151101050374751\n",
      "27 5.140362006977792\n",
      "28 5.130635939560782\n",
      "29 5.121796899011539\n",
      "30 5.113736780747503\n",
      "31 5.106362729782135\n",
      "32 5.09959489731088\n",
      "33 5.093364504542621\n",
      "34 5.087612191493983\n",
      "35 5.08228660714815\n",
      "36 5.07734321701025\n",
      "37 5.072743290699938\n",
      "38 5.068453049234526\n",
      "39 5.064442942322554\n",
      "40 5.060687040377094\n",
      "41 5.057162518612095\n",
      "42 5.053849222637507\n",
      "43 5.050729298616306\n",
      "44 5.04778688115385\n",
      "45 5.04500782630105\n",
      "46 5.0423794855943385\n",
      "47 5.039890511670485\n",
      "48 5.037530693285404\n",
      "49 5.035290812543567\n",
      "50 5.033162523428501\n",
      "51 5.031138246065876\n",
      "52 5.029211076608227\n",
      "53 5.027374708346281\n",
      "54 5.025623364415236\n",
      "55 5.023951738563722\n",
      "56 5.022354944619663\n",
      "57 5.020828471771532\n",
      "58 5.019368146425647\n",
      "59 5.017970098264083\n",
      "60 5.016630731301594\n",
      "61 5.015346697971702\n",
      "62 5.014114877022917\n",
      "63 5.012932353589748\n",
      "64 5.01179640216861\n",
      "65 5.010704471144922\n",
      "66 5.0096541695319114\n",
      "67 5.008643254807953\n",
      "68 5.007669622433391\n",
      "69 5.00673129614132\n",
      "70 5.005826419499602\n",
      "71 5.004953248018056\n",
      "72 5.004110142214952\n",
      "73 5.0032955610713366\n",
      "74 5.0025080562075\n",
      "75 5.001746266342388\n",
      "76 5.001008912296284\n",
      "77 5.0002947922091\n",
      "78 4.999602777168348\n",
      "79 4.998931807011437\n",
      "80 4.9982808864391925\n",
      "81 4.997649081279299\n",
      "82 4.997035514990488\n",
      "83 4.996439365301474\n",
      "84 4.995859861042694\n",
      "85 4.99529627910142\n",
      "86 4.994747941540269\n",
      "87 4.994214212826727\n",
      "88 4.993694497213048\n",
      "89 4.993188236209909\n",
      "90 4.992694906212535\n",
      "91 4.992214016194934\n",
      "92 4.991745105574056\n",
      "93 4.991287742104138\n",
      "94 4.99084151997551\n",
      "95 4.9904060578893645\n",
      "96 4.98998099739107\n",
      "97 4.989566001104263\n",
      "98 4.9891607513019745\n",
      "99 4.988764948275511\n",
      "100 4.988378309150457\n",
      "101 4.988000566360689\n",
      "102 4.987631466721555\n",
      "103 4.98727076997188\n",
      "104 4.986918248124459\n",
      "105 4.986573684028306\n",
      "106 4.986236871028265\n",
      "107 4.985907611484923\n",
      "108 4.985585716792605\n",
      "109 4.985271005772832\n",
      "110 4.9849633051262225\n",
      "111 4.984662447584069\n",
      "112 4.984368272908488\n",
      "113 4.984080625640859\n",
      "114 4.983799356822234\n",
      "115 4.983524321109135\n",
      "116 4.983255379472142\n",
      "117 4.982992395348066\n",
      "118 4.9827352387022295\n",
      "119 4.982483780734077\n",
      "120 4.982237899879429\n",
      "121 4.98199747435845\n",
      "122 4.981762390979483\n",
      "123 4.981532534474898\n",
      "124 4.981307800396735\n",
      "125 4.981088079667931\n",
      "126 4.980873277504578\n",
      "127 4.980663290827293\n",
      "128 4.980458036117567\n",
      "129 4.980257416148665\n",
      "130 4.980061361153451\n",
      "131 4.979869779537048\n",
      "132 4.979682618959076\n",
      "133 4.979499792961828\n",
      "134 4.9793212719429\n",
      "135 4.979146973487965\n",
      "136 4.978976898325567\n",
      "137 4.978810965855097\n",
      "138 4.978649217934893\n",
      "139 4.978491571529171\n",
      "140 4.978338124940272\n",
      "141 4.978188784986\n",
      "142 4.978043728076541\n",
      "143 4.977902837376386\n",
      "144 4.977766397950866\n",
      "145 4.977634246284193\n",
      "146 4.977506818964384\n",
      "147 4.977383867401894\n",
      "148 4.977266039446\n",
      "149 4.977152938697235\n",
      "150 4.977045506714769\n",
      "151 4.976943098847323\n",
      "152 4.97684706271584\n",
      "153 4.9767563482470285\n",
      "154 4.976672859797573\n",
      "155 4.976594902692481\n",
      "156 4.976525136289813\n",
      "157 4.976460870179158\n",
      "158 4.97640577413585\n",
      "159 4.976355670806535\n",
      "160 4.976315562232313\n",
      "161 4.976279141518912\n",
      "162 4.9762531396282395\n",
      "163 4.976228357385226\n",
      "164 4.976213731349276\n",
      "165 4.976196392926976\n",
      "166 4.9761880342107165\n",
      "167 4.976171524673804\n",
      "168 4.976161893197618\n",
      "169 4.976137602680979\n",
      "170 4.976117515894895\n",
      "171 4.976076216415978\n",
      "172 4.976036597968675\n",
      "173 4.975970600771667\n",
      "174 4.9759047739737365\n",
      "175 4.9758101236396675\n",
      "176 4.975715727741554\n",
      "177 4.975593415303806\n",
      "178 4.975473011863765\n",
      "179 4.975328575795408\n",
      "180 4.975188653545396\n",
      "181 4.975030395365809\n",
      "182 4.974879331127362\n",
      "183 4.974716047965626\n",
      "184 4.974561994713792\n",
      "185 4.974401161646323\n",
      "186 4.974250610432174\n",
      "187 4.974097468148113\n",
      "188 4.973954698907789\n",
      "189 4.973812178159363\n",
      "190 4.973679397670041\n",
      "191 4.973548546194019\n",
      "192 4.973426372700452\n",
      "193 4.973306948422537\n",
      "194 4.973194964587944\n",
      "195 4.973085980978588\n",
      "196 4.972983202356374\n",
      "197 4.972883337665123\n",
      "198 4.972788547239125\n",
      "199 4.972696410068952\n",
      "200 4.97260836735723\n",
      "201 4.97252264681895\n",
      "202 4.972440201759458\n",
      "203 4.972359738028485\n",
      "204 4.972281881465857\n",
      "205 4.972205687390296\n",
      "206 4.972131563595834\n",
      "207 4.972058819389485\n",
      "208 4.971987718247095\n",
      "209 4.971917753444908\n",
      "210 4.97184909339015\n",
      "211 4.971781364407048\n",
      "212 4.971714672527176\n",
      "213 4.971648740293643\n",
      "214 4.9715836329637915\n",
      "215 4.971519142748605\n",
      "216 4.9714553083520965\n",
      "217 4.9713919725384805\n",
      "218 4.971329156791598\n",
      "219 4.971266740666352\n",
      "220 4.971204734533743\n",
      "221 4.9711430448170395\n",
      "222 4.971081674758388\n",
      "223 4.971020550478468\n",
      "224 4.970959670680175\n",
      "225 4.970898975981712\n",
      "226 4.970838462222613\n",
      "227 4.970778080732192\n",
      "228 4.970717825565467\n",
      "229 4.970657655991896\n",
      "230 4.970597564972218\n",
      "231 4.9705375176765205\n",
      "232 4.970477506410412\n",
      "233 4.9704175007338804\n",
      "234 4.970357492576383\n",
      "235 4.970297454761498\n",
      "236 4.9702373790224454\n",
      "237 4.970177240601968\n",
      "238 4.970117031160081\n",
      "239 4.9700567277246215\n",
      "240 4.969996321980076\n",
      "241 4.969935792267856\n",
      "242 4.969875130396375\n",
      "243 4.969814315684353\n",
      "244 4.969753340194612\n",
      "245 4.969692184015983\n",
      "246 4.9696308396670466\n",
      "247 4.969569287950256\n",
      "248 4.96950752217666\n",
      "249 4.969445524023892\n",
      "250 4.969383288183595\n",
      "251 4.969320797741627\n",
      "252 4.969258049832938\n",
      "253 4.969195030186496\n",
      "254 4.969131740368665\n",
      "255 4.9690681714030855\n",
      "256 4.969004333153822\n",
      "257 4.968940227597253\n",
      "258 4.968875880745427\n",
      "259 4.968811317865907\n",
      "260 4.968746597824152\n",
      "261 4.968681797071531\n",
      "262 4.968617044700184\n",
      "263 4.968552534289963\n",
      "264 4.968488553258743\n",
      "265 4.9684255763842184\n",
      "266 4.968364268460676\n",
      "267 4.968305816954691\n",
      "268 4.9682518378667195\n",
      "269 4.968205433716801\n",
      "270 4.968170745880174\n",
      "271 4.968156335271425\n",
      "272 4.9681733069620035\n",
      "273 4.968246587447107\n",
      "274 4.968406389583819\n",
      "275 4.9687276260318365\n",
      "276 4.969285330607429\n",
      "277 4.97029456710783\n",
      "278 4.971852908307564\n",
      "279 4.974422803632844\n",
      "280 4.977475103110635\n",
      "281 4.981383951676089\n",
      "282 4.983110255402158\n",
      "283 4.983980832889061\n",
      "284 4.980976915430036\n",
      "285 4.978435436295185\n",
      "286 4.974964930893537\n",
      "287 4.972824127743076\n",
      "288 4.970965826440318\n",
      "289 4.969861539403999\n",
      "290 4.969009974041654\n",
      "291 4.968475894523946\n",
      "292 4.9680649610387375\n",
      "293 4.967782850876916\n",
      "294 4.967554721290795\n",
      "295 4.967381075393514\n",
      "296 4.9672309063907685\n",
      "297 4.967105427550859\n",
      "298 4.966990333619228\n",
      "299 4.966887510340329\n",
      "300 4.966789459222982\n",
      "301 4.9666984972181885\n",
      "302 4.966610116931284\n",
      "303 4.966527169763143\n",
      "304 4.966446600191042\n",
      "305 4.9663723295376085\n",
      "306 4.966302214658258\n",
      "307 4.966242594551118\n",
      "308 4.966192421109065\n",
      "309 4.966163843730015\n",
      "310 4.966158235200608\n",
      "311 4.966202681850883\n",
      "312 4.966304694895401\n",
      "313 4.966531963880123\n",
      "314 4.966903684153652\n",
      "315 4.967597297288986\n",
      "316 4.96861424810048\n",
      "317 4.970376496228569\n",
      "318 4.972508334851971\n",
      "319 4.9756963432931025\n",
      "320 4.977775506504898\n",
      "321 4.979959864285063\n",
      "322 4.978511643383169\n",
      "323 4.977260092872655\n",
      "324 4.973957557775948\n",
      "325 4.9719478621272035\n",
      "326 4.969781438309008\n",
      "327 4.968548705006483\n",
      "328 4.967482170677544\n",
      "329 4.96684599538374\n",
      "330 4.9663177718801546\n",
      "331 4.9659739702309595\n",
      "332 4.965681761405345\n",
      "333 4.965475174146043\n",
      "334 4.965295299823092\n",
      "335 4.965165434782628\n",
      "336 4.96505934568868\n",
      "337 4.965000747533481\n",
      "338 4.964988942552676\n",
      "339 4.965062297848213\n",
      "340 4.9652762750015675\n",
      "341 4.965724464386411\n",
      "342 4.9666705842422685\n",
      "343 4.968328761600704\n",
      "344 4.971674624523452\n",
      "345 4.97599427190207\n",
      "346 4.983136705194377\n",
      "347 4.983932450478482\n",
      "348 4.985776648356911\n",
      "349 4.977431955613025\n",
      "350 4.974646321488065\n",
      "351 4.970036728123678\n",
      "352 4.968496085236346\n",
      "353 4.966844231783738\n",
      "354 4.966219143219925\n",
      "355 4.965561513106908\n",
      "356 4.965294678599212\n",
      "357 4.964982066786053\n",
      "358 4.964881077023663\n",
      "359 4.96473277089034\n",
      "360 4.964767835170839\n",
      "361 4.964761397039608\n",
      "362 4.965001326696643\n",
      "363 4.965207205984666\n",
      "364 4.9658483404696945\n",
      "365 4.9664240628748555\n",
      "366 4.967832643174367\n",
      "367 4.968859332168625\n",
      "368 4.971272794997878\n",
      "369 4.971989573569582\n",
      "370 4.974384535332491\n",
      "371 4.9730838015335905\n",
      "372 4.973695017747281\n",
      "373 4.970953048520704\n",
      "374 4.970273736295473\n",
      "375 4.968087815449754\n",
      "376 4.967349244225987\n",
      "377 4.96609181763627\n",
      "378 4.965635561884369\n",
      "379 4.964971000260157\n",
      "380 4.964767511998256\n",
      "381 4.964455445374855\n",
      "382 4.964489563832764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 4.964480972525233\n",
      "384 4.964870156855731\n",
      "385 4.9653315219153935\n",
      "386 4.966449604964184\n",
      "387 4.967863449997844\n",
      "388 4.9703555996407065\n",
      "389 4.973087646931596\n",
      "390 4.976229502705177\n",
      "391 4.9779164611015325\n",
      "392 4.977548796351662\n",
      "393 4.975739285717876\n",
      "394 4.972880114230659\n",
      "395 4.970826307213733\n",
      "396 4.968939149278973\n",
      "397 4.967944148199288\n",
      "398 4.967104757913052\n",
      "399 4.966785383803584\n",
      "400 4.9665286745001715\n",
      "401 4.9665965938603165\n",
      "402 4.966712227161048\n",
      "403 4.967038629461489\n",
      "404 4.967426700765185\n",
      "405 4.9678630450593655\n",
      "406 4.968363956931493\n",
      "407 4.968621821828347\n",
      "408 4.968965267434685\n",
      "409 4.968740662933588\n",
      "410 4.968745981635856\n",
      "411 4.968048894921792\n",
      "412 4.967822333253995\n",
      "413 4.966963744661243\n",
      "414 4.9667426739494855\n",
      "415 4.965993426838553\n",
      "416 4.9659410671224835\n",
      "417 4.96540201294115\n",
      "418 4.965605440115417\n",
      "419 4.965281331689166\n",
      "420 4.965818634879133\n",
      "421 4.965669339284441\n",
      "422 4.9666214243437175\n",
      "423 4.9665132218038055\n",
      "424 4.967866809701262\n",
      "425 4.967476265445866\n",
      "426 4.96897019671411\n",
      "427 4.967929265315995\n",
      "428 4.969171127014522\n",
      "429 4.967561863820994\n",
      "430 4.968452129035345\n",
      "431 4.966792701684305\n",
      "432 4.967561469299175\n",
      "433 4.966217445774179\n",
      "434 4.9671218106293\n",
      "435 4.966138514219075\n",
      "436 4.967328989820466\n",
      "437 4.966572660977314\n",
      "438 4.968047895555901\n",
      "439 4.96727196773446\n",
      "440 4.9687962718378245\n",
      "441 4.967731056019673\n",
      "442 4.968909790948388\n",
      "443 4.967522398489235\n",
      "444 4.96816040023972\n",
      "445 4.966738141605475\n",
      "446 4.967004213187232\n",
      "447 4.965848935876444\n",
      "448 4.966033148259272\n",
      "449 4.965266703133827\n",
      "450 4.96558620080667\n",
      "451 4.965224636216726\n",
      "452 4.96582188805222\n",
      "453 4.965865608976155\n",
      "454 4.96681358413755\n",
      "455 4.967213497373855\n",
      "456 4.968382038218725\n",
      "457 4.9688811051700394\n",
      "458 4.969760807780791\n",
      "459 4.9699078288482195\n",
      "460 4.9699544372669875\n",
      "461 4.9696121987220305\n",
      "462 4.96894492986407\n",
      "463 4.96845579918117\n",
      "464 4.9676001823565\n",
      "465 4.9672843938430296\n",
      "466 4.966565434818554\n",
      "467 4.96653298477346\n",
      "468 4.966030580975135\n",
      "469 4.966294578222328\n",
      "470 4.965975064607766\n",
      "471 4.966515281901694\n",
      "472 4.9662756427743835\n",
      "473 4.96702358840871\n",
      "474 4.96669323518834\n",
      "475 4.967521913473143\n",
      "476 4.966931011967934\n",
      "477 4.9677362629730695\n",
      "478 4.966862095589946\n",
      "479 4.967687757206055\n",
      "480 4.9666903729565\n",
      "481 4.967733962954173\n",
      "482 4.966801351152587\n",
      "483 4.968306172382619\n",
      "484 4.967481742274681\n",
      "485 4.969580633822016\n",
      "486 4.968634388143222\n",
      "487 4.971061329597996\n",
      "488 4.969515319237755\n",
      "489 4.9714568976448135\n",
      "490 4.969173924258029\n",
      "491 4.97000286553634\n",
      "492 4.967626204566904\n",
      "493 4.967627962080935\n",
      "494 4.96580551110935\n",
      "495 4.9656006103853985\n",
      "496 4.964465077454275\n",
      "497 4.964416221989251\n",
      "498 4.963862552129322\n",
      "499 4.964113190052232\n"
     ]
    }
   ],
   "source": [
    "# Construct an Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y1_pred = model(x1)\n",
    "    y2_pred = model(x2)\n",
    "    \n",
    "    outputs = torch.cat((y1_pred, y2_pred), 0)\n",
    "    \n",
    "    # Project the output onto Stiefel Manifold\n",
    "    u, s, v = torch.svd(outputs, some=True)\n",
    "    proj_outputs = u@v.t()\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = torch.trace(proj_outputs.t()@L@proj_outputs)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    proj_outputs.retain_grad()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # Project the (Euclidean) gradient onto the tangent space of Stiefel Manifold (to get Rimannian gradient)\n",
    "    rgrad = proj_stiefel(proj_outputs, proj_outputs.grad) \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # Backpropogate the Rimannian gradient w.r.t proj_outputs\n",
    "    proj_outputs.backward(rgrad)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Stuff - Ignore them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 1,  1],\n",
       "       [ 2,  2],\n",
       "       [ 3,  3],\n",
       "       [ 4,  4],\n",
       "       [ 5,  5],\n",
       "       [ 6,  6],\n",
       "       [ 7,  7],\n",
       "       [ 8,  8],\n",
       "       [ 9,  9],\n",
       "       [10, 10],\n",
       "       [11, 11],\n",
       "       [12, 12],\n",
       "       [13, 13],\n",
       "       [14, 14],\n",
       "       [15, 15],\n",
       "       [16, 16],\n",
       "       [17, 17],\n",
       "       [18, 18],\n",
       "       [19, 19],\n",
       "       [20, 20],\n",
       "       [21, 21],\n",
       "       [22, 22],\n",
       "       [23, 23],\n",
       "       [24, 24],\n",
       "       [25, 25],\n",
       "       [26, 26],\n",
       "       [27, 27],\n",
       "       [28, 28],\n",
       "       [29, 29],\n",
       "       [30, 30],\n",
       "       [31, 31],\n",
       "       [32, 32],\n",
       "       [33, 33],\n",
       "       [34, 34],\n",
       "       [35, 35],\n",
       "       [36, 36],\n",
       "       [37, 37],\n",
       "       [38, 38],\n",
       "       [39, 39],\n",
       "       [40, 40],\n",
       "       [41, 41],\n",
       "       [42, 42],\n",
       "       [43, 43],\n",
       "       [44, 44],\n",
       "       [45, 45],\n",
       "       [46, 46],\n",
       "       [47, 47],\n",
       "       [48, 48],\n",
       "       [49, 49],\n",
       "       [50, 50],\n",
       "       [51, 51],\n",
       "       [52, 52],\n",
       "       [53, 53],\n",
       "       [54, 54],\n",
       "       [55, 55],\n",
       "       [56, 56],\n",
       "       [57, 57],\n",
       "       [58, 58],\n",
       "       [59, 59],\n",
       "       [60, 60],\n",
       "       [61, 61],\n",
       "       [62, 62],\n",
       "       [63, 63]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr.pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: Compute predicted y by passing x to the model\n",
    "y1_pred = model(x1)\n",
    "y2_pred = model(x2)\n",
    "\n",
    "# Compute and print loss\n",
    "#     loss = loss_fn(y1_pred, y2_pred)\n",
    "outputs = torch.cat((y1_pred, y2_pred), 0)\n",
    "loss = torch.trace(outputs.t()@L@outputs)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgrad = proj_stiefel(outputs,outputs.grad)\n",
    "rgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.backward(rgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print (p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs1, outputs2):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs and labels.\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size x 6 - output of the model\n",
    "        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n",
    "    Returns:\n",
    "        loss (Variable): cross entropy loss for all images in the batch\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "    outputs = torch.cat((outputs1, outputs2), 0)\n",
    "    return outputs.t()@L@outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprod(A, B):\n",
    "    # Added just to be parallel to manopt/pymanopt implemenetation\n",
    "    return torch.matmul(A, B)\n",
    "def multisym(A):\n",
    "    # Inspired by MATLAB multisym function by Nicholas Boumal.\n",
    "    return 0.5 * (A + multitransp(A))\n",
    "def multitransp(A):\n",
    "    # First check if we have been given just one matrix\n",
    "    if A.dim() == 2:\n",
    "        return torch.transpose(A, 1, 0)\n",
    "    return torch.transpose(A, 2, 1)\n",
    "\n",
    "outputs.grad - multiprod(outputs, multisym(multiprod(multitransp(outputs), outputs.grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.DoubleTensor([2]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2\n",
    "z = y**2-1\n",
    "y.retain_grad()\n",
    "z.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.DoubleTensor([54]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian(torch.DoubleTensor([[0,1,0],[1,0,1],[0,1,0]]), normed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x -= x.grad\n",
    "y -=y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(mNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        y_pred = self.linear2(h)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 1, 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = mNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mmodel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mmodel.parameters():\n",
    "    print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in mmodel.parameters():\n",
    "    print (p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.linalg.eigvals(laplacian(arr, normed=False)) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(laplacian(arr, normed=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[0,1,0],[1,0,1],[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(laplacian(arr, normed=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def isPSD(A, tol=1e-8):\n",
    "  E,V = scipy.linalg.eigh(A)\n",
    "  return np.all(E > -tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isPSD(laplacian(arr, normed=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
